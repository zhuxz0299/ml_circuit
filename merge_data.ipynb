{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "import pickle\n",
    "\n",
    "class CustomDataset(InMemoryDataset):\n",
    "    def __init__(self, root, data_list, transform=None, pre_transform=None):\n",
    "        self.data_list = data_list\n",
    "        super(CustomDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = self.collate(data_list)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return []\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph_folder = 'train_graph'\n",
    "train_label_folder = 'train_label'\n",
    "\n",
    "pt_files = [os.path.join(train_graph_folder, f) for f in os.listdir(train_graph_folder) if f.endswith('.pt')]\n",
    "pkl_files = [os.path.join(train_label_folder, f) for f in os.listdir(train_label_folder) if f.endswith('.pkl')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging files: 100%|██████████| 441212/441212 [03:55<00:00, 1876.68it/s]\n"
     ]
    }
   ],
   "source": [
    "all_data = []\n",
    "all_labels = []\n",
    "\n",
    "for pt_file, pkl_file in tqdm(zip(pt_files, pkl_files), desc=\"Merging files\", total=len(pt_files)):\n",
    "    data = torch.load(pt_file)\n",
    "    with open(pkl_file, 'rb') as f:\n",
    "        label = pickle.load(f)\n",
    "\n",
    "    all_data.append(data)\n",
    "    all_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 441212/441212 [00:12<00:00, 34914.57it/s]\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "output_graph_file = 'train_graph.pt'\n",
    "all_data_formal = []\n",
    "for data in tqdm(all_data):\n",
    "    if isinstance(data, dict):\n",
    "        data = Data(**data)\n",
    "    all_data_formal.append(data)\n",
    "# 保存合并后的图数据到一个文件中\n",
    "dataset = CustomDataset('', all_data_formal)\n",
    "torch.save(dataset, output_graph_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "output_label_file = 'train_label.pkl'\n",
    "# Save merged labels to a file\n",
    "with open(output_label_file, 'wb') as f:\n",
    "    pickle.dump(all_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging files: 100%|██████████| 441212/441212 [05:40<00:00, 1294.85it/s]\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged graph data saved to data/merged_graph_data.pt\n",
      "Merged labels saved to data/merged_labels.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "import pickle\n",
    "\n",
    "class CustomDataset(InMemoryDataset):\n",
    "    def __init__(self, root, data_list, transform=None, pre_transform=None):\n",
    "        self.data_list = data_list\n",
    "        super(CustomDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = self.collate(data_list)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return []\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        pass\n",
    "\n",
    "def convert_dict_to_data(data_dict):\n",
    "    node_type = data_dict['node_type'].float().unsqueeze(1)  # Add feature dimension and convert to float\n",
    "    num_inverted_predecessors = data_dict['num_inverted_predecessors'].float().unsqueeze(1)  # Add feature dimension and convert to float\n",
    "    node_features = torch.cat([node_type, num_inverted_predecessors], dim=1)\n",
    "    edge_index = data_dict['edge_index']\n",
    "    data = Data(x=node_features, edge_index=edge_index)\n",
    "    return data\n",
    "\n",
    "def merge_datasets(train_graph_folder, train_label_folder, output_graph_file, output_label_file):\n",
    "    pt_files = sorted([os.path.join(train_graph_folder, f) for f in os.listdir(train_graph_folder) if f.endswith('.pt')])\n",
    "    pkl_files = sorted([os.path.join(train_label_folder, f) for f in os.listdir(train_label_folder) if f.endswith('.pkl')])\n",
    "\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "\n",
    "    for pt_file, pkl_file in tqdm(zip(pt_files, pkl_files), desc=\"Merging files\", total=len(pt_files)):\n",
    "        data_dict = torch.load(pt_file)\n",
    "        data = convert_dict_to_data(data_dict)\n",
    "        all_data.append(data)\n",
    "\n",
    "        with open(pkl_file, 'rb') as f:\n",
    "            label = pickle.load(f)\n",
    "        all_labels.append(label)\n",
    "\n",
    "    dataset = CustomDataset('', all_data)\n",
    "    torch.save(dataset, output_graph_file)\n",
    "\n",
    "    with open(output_label_file, 'wb') as f:\n",
    "        pickle.dump(all_labels, f)\n",
    "\n",
    "    print(f'Merged graph data saved to {output_graph_file}')\n",
    "    print(f'Merged labels saved to {output_label_file}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_graph_folder = 'tmp_data/train_graph'\n",
    "    train_label_folder = 'tmp_data/train_label'\n",
    "    output_graph_file = 'data/merged_graph_data.pt'\n",
    "    output_label_file = 'data/merged_labels.pkl'\n",
    "    merge_datasets(train_graph_folder, train_label_folder, output_graph_file, output_label_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.033740330807631835\n",
      "Epoch 2, Loss: 0.011013712850399315\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "# Load merged graph data and labels\n",
    "dataset = torch.load('data/merged_graph_data.pt')\n",
    "with open('data/merged_labels.pkl', 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "# Create data loader\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "        self.fc = torch.nn.Linear(out_channels, 1)  # Output is a single number\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = torch.mean(x, dim=0)  # Mean of node features\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = GNN(in_channels=2, hidden_channels=64, out_channels=32)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(2):\n",
    "    total_loss = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        target = torch.tensor([labels[i]], dtype=torch.float32)\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
